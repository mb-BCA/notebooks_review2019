{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification to identify tasks\n",
    "\n",
    "This notebook reproduces plots that appear in Fig 4 of the paper:\n",
    "- Gilson M, Zamora-López G, Pallarés V, Adhikari MH, Senden M, Tauste Campo A, Mantini D, Corbetta M, Deco G, Insabato A (submitted) \"Model-based whole-brain effective connectivity to study distributed cognition in health and disease\", bioRxiv; https://doi.org/10.1101/531830.\n",
    "\n",
    "The goal of to compare the connectivity measures (in particular, effective versus functional connectivity) in identifying tasks performed by subjects in the scanner. The data comprise 22 subjects with 5 sessions each (2 for resting-state and 3 for movie, with distinct parts of the movie M1-M2-M3). Two classifications are considered: rest versus movie, and rest/M1/M2/M3.\n",
    "\n",
    "It uses the scikit-learn library, see https://scikit-learn.org/ for details and tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle to True to create directory and store results there\n",
    "save_outputs = False\n",
    "if save_outputs:\n",
    "    import os\n",
    "    res_dir = 'classif/'\n",
    "    if not os.path.exists(res_dir):\n",
    "        os.mkdir(res_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data and calculate the features: 'EC' stands for effective connectivity (estimated for the MOU model, see the *MOU_EC_Estimation* notebook), 'FC' for the BOLD correlations, 'FC + mask' for the BOLD correlations only concerning the ROI connections that exist according to the structural data (diffusion tensor imaging), 'PC' for the partial correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import scipy.stats as stt\n",
    "import sklearn.linear_model as skllm\n",
    "import sklearn.neighbors as sklnn\n",
    "import sklearn.discriminant_analysis as skda\n",
    "import sklearn.preprocessing as skppc\n",
    "import sklearn.pipeline as skppl\n",
    "import sklearn.metrics as skm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data parameters\n",
    "param_dir = 'model_param_movie/'\n",
    "n_sub = 22 # number of subjects\n",
    "n_run = 5  # number of sessions per subject (2 rest + 3 movie)\n",
    "N = 66     # number of ROIs\n",
    "\n",
    "# Load data previously generated in notebook 'MOU_EC_Estimation.ipynb'\n",
    "# Effective connectivity estimated using the MOU dynamic model\n",
    "EC = np.load(param_dir + 'J_mod.npy')\n",
    "# Mask of existing connections in SC (structural connectivity)\n",
    "mask_EC = np.load(param_dir + 'mask_EC.npy')\n",
    "# BOLD covariances (without time lag)\n",
    "FC0 = np.load(param_dir + 'FC_emp.npy')[:,:,0,:,:]\n",
    "# Triangular mask to retain half of the matrix elemenets in symmetric matrices\n",
    "mask_tri = np.tri(N,N,-1,dtype=bool)\n",
    "\n",
    "# Calculate features\n",
    "for i_sub in range(n_sub):\n",
    "    for i_run in range(n_run):\n",
    "        # apply z-scoring to EC\n",
    "        EC[i_sub,i_run,mask_EC] = stt.zscore(EC[i_sub,i_run,mask_EC])\n",
    "\n",
    "corr = np.copy(FC0)\n",
    "for i_sub in range(n_sub):\n",
    "    for i_run in range(n_run):\n",
    "        corr[i_sub,i_run,:,:] /= np.sqrt( np.outer(corr[i_sub,i_run,:,:].diagonal(), \n",
    "                                                   corr[i_sub,i_run,:,:].diagonal()) )\n",
    "\n",
    "PC = np.copy(FC0)\n",
    "for i_sub in range(n_sub):\n",
    "    for i_run in range(n_run):\n",
    "        PC[i_sub,i_run,:,:] = -np.linalg.pinv(PC[i_sub,i_run,:,:])\n",
    "        PC[i_sub,i_run,:,:] /= np.sqrt( np.outer(PC[i_sub,i_run,:,:].diagonal(), \n",
    "                                                 PC[i_sub,i_run,:,:].diagonal()) )\n",
    "\n",
    "# Plotting labels for connectivity measures\n",
    "n_conn = 4\n",
    "label_conn = ['EC', 'FC', 'FC + mask', 'PC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code repeats the classification by splitting the fMRI sessions into a train set and test set (`n_rep` times). Here the 22 subjects are split in 80% for training and 20% for testing. We compare 3 classifiers:\n",
    "\n",
    "- MLR is the multinomial logistic regression;\n",
    "- 1NN is the 1-nearest-neighbor;\n",
    "- LDA performs the linear discriminant analysis.\n",
    "\n",
    "See the scikit-learn webpage for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: Running this cell will take a few minutes\n",
    "# Labels of sessions for classification\n",
    "# distinct labels for movie runs\n",
    "task4_labels = np.repeat( np.array([0,0,1,2,3], dtype=int).reshape([1,-1]), \n",
    "                          n_sub, \n",
    "                          axis=0 )\n",
    "# rest versus movie, same labels for movie runs\n",
    "RM_labels = np.repeat( np.array([0,0,1,1,1], dtype=int).reshape([1,-1]), \n",
    "                       n_sub, \n",
    "                       axis=0 ) \n",
    "\n",
    "# number of repetitions and storage of results\n",
    "n_rep = 40\n",
    "# first index: task4_labels/RM_labels; last index: MRL/1NN/LDA\n",
    "perf = np.zeros([2,n_rep,n_conn,3])\n",
    "\n",
    "\n",
    "# Loop over classification type\n",
    "for i_classif in range(2):\n",
    "\n",
    "    if i_classif==0:\n",
    "        # discriminate rest and 3 movie sessions individually\n",
    "        lab_tmp = task4_labels\n",
    "        print( 'Running classification: tasks ...' )\n",
    "    else:\n",
    "        # discriminate rest vs movie\n",
    "        lab_tmp = RM_labels\n",
    "        print( '\\nRunning classification: rest vs. movie ...' )\n",
    "\n",
    "    # Repeat classification\n",
    "    for i_rep in range(n_rep):\n",
    "        print( '\\tRepetition %d' %i_rep )\n",
    "        \n",
    "        # Split run indices in train and test sets \n",
    "        # (choose some subjects for train and rest for test)\n",
    "        train_ind = np.ones([n_sub,n_run], dtype=bool)\n",
    "        while train_ind[:,0].sum() >= 0.8 * n_sub:\n",
    "            train_ind[np.random.randint(n_sub),:] = False\n",
    "        test_ind = np.logical_not(train_ind)        \n",
    "\n",
    "        # Loop over connectivity measures\n",
    "        for i_conn in range(n_conn):\n",
    "\n",
    "            if i_conn==0:\n",
    "                # vectorized EC matrices (only retaining existing connections)\n",
    "                vect_features = EC[:,:,mask_EC]\n",
    "            elif i_conn==1:\n",
    "                # vectorized FC matrices (only retaining low triangle)\n",
    "                vect_features = corr[:,:,mask_tri]\n",
    "            elif i_conn==2:\n",
    "                # vectorized FC matrices (only retaining SC existing connections)\n",
    "                vect_features = corr[:,:,mask_EC]\n",
    "            else:\n",
    "                # vectorized PCmatrices (only retaining low triangle)\n",
    "                vect_features = PC[:,:,mask_tri]\n",
    "\n",
    "            # dimension of vectorized EC\n",
    "            dim_feature = vect_features.shape[2]\n",
    "\n",
    "            # Number of components for LDA\n",
    "            if i_classif==0:\n",
    "                n_comp_LDA = 3\n",
    "            else:\n",
    "                n_comp_LDA = 1\n",
    "\n",
    "            # Classifier and learning parameters\n",
    "            c_MLR = skppl.make_pipeline( skppc.StandardScaler(), \n",
    "                                        skllm.LogisticRegression(C=0.1, \n",
    "                                                                 penalty='l2', \n",
    "                                                                 multi_class='multinomial', \n",
    "                                                                 solver='lbfgs', \n",
    "                                                                 max_iter=500) )\n",
    "            c_1NN = sklnn.KNeighborsClassifier(n_neighbors=1, \n",
    "                                               algorithm='brute', \n",
    "                                               metric='correlation')\n",
    "            c_LDA = skda.LinearDiscriminantAnalysis(n_components=n_comp_LDA, \n",
    "                                                    solver='eigen', \n",
    "                                                    shrinkage='auto')\n",
    "            \n",
    "            # Train and test classifiers with subject labels\n",
    "            c_MLR.fit(vect_features[train_ind,:], lab_tmp[train_ind])\n",
    "            perf[i_classif, i_rep,i_conn,0] = \\\n",
    "                c_MLR.score( vect_features[test_ind,:], lab_tmp[test_ind] )\n",
    "        \n",
    "            c_1NN.fit(vect_features[train_ind,:], lab_tmp[train_ind])\n",
    "            perf[i_classif, i_rep,i_conn,1] = \\\n",
    "                c_1NN.score(vect_features[test_ind,:], lab_tmp[test_ind])\n",
    "        \n",
    "            c_LDA.fit(vect_features[train_ind,:], lab_tmp[train_ind])\n",
    "            perf[i_classif, i_rep,i_conn,2] = \\\n",
    "                c_LDA.score(vect_features[test_ind,:], lab_tmp[test_ind])\n",
    "\n",
    "# Save results\n",
    "if save_outputs:\n",
    "    np.save(res_dir + 'perf_tasks.npy', perf)\n",
    "\n",
    "print( '\\nFinished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is interpreted by comparing the classification accuracy with the chance level (the dashed line), in relation with the gap towards 100% correct. Here we can calculate the chance level straight from the session properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rest versus movie\n",
    "# chance level: predicting always movie gives 60% (as compared to predicting rest, 40%)\n",
    "chance_level = 3. / n_run \n",
    "\n",
    "plt.figure()\n",
    "plt.violinplot(perf[1,:,:,0], positions=np.arange(n_conn)-0.25, widths=[0.25]*n_conn)\n",
    "plt.violinplot(perf[1,:,:,1], positions=np.arange(n_conn), widths=[0.25]*n_conn)\n",
    "plt.violinplot(perf[1,:,:,2], positions=np.arange(n_conn)+0.25, widths=[0.25]*n_conn)\n",
    "plt.plot([-1,n_conn], [chance_level]*2, '--k')\n",
    "plt.axis(xmin=-0.6, xmax=n_conn-0.4, ymin=0, ymax=1.01)\n",
    "plt.xticks(range(n_conn), ['EC','FC','FC+mask','PC'], rotation=20)\n",
    "plt.yticks([0,1])\n",
    "plt.ylabel('accuracy', fontsize=14)\n",
    "plt.title('rest versus movie', fontsize=14)\n",
    "if save_outputs:\n",
    "    plt.savefig(res_dir + 'perf_RM.png', format='png')\n",
    "\n",
    "# Plot 4 classes (rest, movie1, movie2, movie3)\n",
    "# Chance level: predicting always rest gives 40% (as compared to predicting \n",
    "# a movie session, 20%)\n",
    "chance_level = 2. / n_run \n",
    "\n",
    "plt.figure()\n",
    "plt.violinplot(perf[0,:,:,0], positions=np.arange(n_conn)-0.25, widths=[0.25]*n_conn)\n",
    "plt.violinplot(perf[0,:,:,1], positions=np.arange(n_conn), widths=[0.25]*n_conn)\n",
    "plt.violinplot(perf[0,:,:,2], positions=np.arange(n_conn)+0.25, widths=[0.25]*n_conn)\n",
    "plt.plot([-1,n_conn], [chance_level]*2, '--k')\n",
    "plt.axis(xmin=-0.6,xmax=n_conn-0.4,ymin=0,ymax=1.01)\n",
    "plt.xticks(range(n_conn), ['EC','FC','FC+mask','PC'], rotation=20, fontsize=14)\n",
    "plt.yticks([0,1])\n",
    "plt.ylabel('accuracy', fontsize=14)\n",
    "plt.title('4-task classification', fontsize=14)\n",
    "if save_outputs:\n",
    "    plt.savefig(res_dir + 'perf_4tasks.png', format='png')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Confusion matrices** are useful to understand the errors performed by the train classifier on the test set. Each row represents the classification of a test sample, with the diagonal element corresponding to the correct classification and off-diagonal elements to errors. Here we separate the 5 sessions to show that the two rest sessions are not separable by the MRL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use vectorized EC as features\n",
    "vect_features = EC[:,:,mask_EC]\n",
    "                   \n",
    "# Individual labels for each session for classification (separating the 2 rest sessions)\n",
    "# Distinct labels for each run/session\n",
    "run5_labels = np.repeat( np.array([0,1,2,3,4],dtype=int).reshape([1,-1]), \n",
    "                         n_sub, \n",
    "                         axis=0 ) \n",
    "\n",
    "# Confusion matrices (1 dimension per condition)\n",
    "conf_mat_MLR = np.zeros([n_rep,n_run,n_run])\n",
    "conf_mat_1NN = np.zeros([n_rep,n_run,n_run])\n",
    "\n",
    "# Perform classification\n",
    "for i_rep in range(n_rep):\n",
    "\n",
    "    # Split run indices in train and test sets \n",
    "    # (choose some subjects for train and rest for test)\n",
    "    train_ind = np.ones([n_sub,n_run],dtype=bool)\n",
    "    while train_ind[:,0].sum() >= 0.8*n_sub:\n",
    "        train_ind[np.random.randint(n_sub),:] = False\n",
    "    test_ind = np.logical_not(train_ind)\n",
    "    \n",
    "    # Train and test classifiers with subject labels\n",
    "    c_MLR.fit( vect_features[train_ind,:], run5_labels[train_ind] )\n",
    "    conf_mat_MLR[i_rep,:,:] = \\\n",
    "            skm.confusion_matrix( y_true=run5_labels[test_ind], \n",
    "                                  y_pred=c_MLR.predict(vect_features[test_ind,:]) )\n",
    "\n",
    "    c_1NN.fit( vect_features[train_ind,:], run5_labels[train_ind] )\n",
    "    conf_mat_1NN[i_rep,:,:] = \\\n",
    "            skm.confusion_matrix( y_true=run5_labels[test_ind], \n",
    "                                  y_pred=c_1NN.predict(vect_features[test_ind,:]) )\n",
    "\n",
    "# Plots\n",
    "plt.figure()\n",
    "plt.imshow(conf_mat_MLR.mean(0), origin='lower', cmap='Greens', vmin=0, vmax=5)\n",
    "plt.xticks(range(n_run), ['R1','R2','M1','M2','M3'])\n",
    "plt.yticks(range(n_run), ['R1','R2','M1','M2','M3'])\n",
    "plt.colorbar(ticks=range(6))\n",
    "plt.xlabel('Predicted condition', fontsize=14)\n",
    "plt.ylabel('True condition', fontsize=14)\n",
    "plt.title('MLR + EC', fontsize=14)\n",
    "if save_outputs:\n",
    "    plt.savefig(res_dir + 'conf_mat_MLR_EC.png', format='png')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(conf_mat_1NN.mean(0), origin='lower', cmap='Greens', vmin=0, vmax=5)\n",
    "plt.xticks(range(n_run), ['R1','R2','M1','M2','M3'])\n",
    "plt.yticks(range(n_run), ['R1','R2','M1','M2','M3'])\n",
    "plt.colorbar(ticks=range(6))\n",
    "plt.xlabel('Predicted condition', fontsize=14)\n",
    "plt.ylabel('True condition', fontsize=14)\n",
    "plt.title('1NN + EC', fontsize=14)\n",
    "if save_outputs:\n",
    "    plt.savefig(res_dir + 'conf_mat_1NN_EC.png', format='png')\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
